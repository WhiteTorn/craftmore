{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOsS/+ZjmpsDR11P7lh45NR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rToK0Tku8PPn"},"source":["## makemore: becoming a backprop ninja"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"8sFElPqq8PPp","executionInfo":{"status":"ok","timestamp":1762963342141,"user_tz":-240,"elapsed":44,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["# there no change in the first several cells from last lecture\n","# I am about to become backprop ninja, vuhu"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ChBbac4y8PPq","executionInfo":{"status":"ok","timestamp":1762963350828,"user_tz":-240,"elapsed":8680,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt # for making figures\n","%matplotlib inline"]},{"cell_type":"code","source":["# download the names.txt file from github\n","!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"],"metadata":{"id":"x6GhEWW18aCS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963351128,"user_tz":-240,"elapsed":271,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"305c8872-1d32-44d7-c485-4091b80b9231"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-11-12 16:02:30--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 228145 (223K) [text/plain]\n","Saving to: ‘names.txt’\n","\n","\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n","\n","2025-11-12 16:02:30 (8.11 MB/s) - ‘names.txt’ saved [228145/228145]\n","\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"klmu3ZG08PPr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963351145,"user_tz":-240,"elapsed":12,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"432c92f8-684a-4428-a4b1-a2d85ac88bcd"},"outputs":[{"output_type":"stream","name":"stdout","text":["32033\n","15\n","['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"]}],"source":["# read in all the words\n","words = open('names.txt', 'r').read().splitlines()\n","print(len(words))\n","print(max(len(w) for w in words))\n","print(words[:8])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"BCQomLE_8PPs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963351162,"user_tz":-240,"elapsed":13,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"24a59999-c725-43c0-aa6f-9666ecc2b4c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n","27\n"]}],"source":["# build the vocabulary of characters and mappings to/from integers\n","chars = sorted(list(set(''.join(words))))\n","stoi = {s:i+1 for i,s in enumerate(chars)}\n","stoi['.'] = 0\n","itos = {i:s for s,i in stoi.items()}\n","vocab_size = len(itos)\n","print(itos)\n","print(vocab_size)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"V_zt2QHr8PPs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963352067,"user_tz":-240,"elapsed":902,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"a4407ab0-8045-4583-8d13-fae93d4d59db"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([182625, 3]) torch.Size([182625])\n","torch.Size([22655, 3]) torch.Size([22655])\n","torch.Size([22866, 3]) torch.Size([22866])\n"]}],"source":["# build the dataset\n","block_size = 3 # context length: how many characters do we take to predict the next one?\n","\n","def build_dataset(words):\n","  X, Y = [], []\n","\n","  for w in words:\n","    context = [0] * block_size\n","    for ch in w + '.':\n","      ix = stoi[ch]\n","      X.append(context)\n","      Y.append(ix)\n","      context = context[1:] + [ix] # crop and append\n","\n","  X = torch.tensor(X)\n","  Y = torch.tensor(Y)\n","  print(X.shape, Y.shape)\n","  return X, Y\n","\n","import random\n","random.seed(42)\n","random.shuffle(words)\n","n1 = int(0.8*len(words))\n","n2 = int(0.9*len(words))\n","\n","Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n","Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n","Xte,  Yte  = build_dataset(words[n2:])     # 10%"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eg20-vsg8PPt","executionInfo":{"status":"ok","timestamp":1762963352072,"user_tz":-240,"elapsed":11,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["# ok biolerplate done, now we get to the action:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"MJPU8HT08PPu","executionInfo":{"status":"ok","timestamp":1762963352076,"user_tz":-240,"elapsed":6,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["# utility function we will use later when comparing manual gradients to PyTorch gradients\n","def cmp(s, dt, t):\n","  ex = torch.all(dt == t.grad).item()\n","  app = torch.allclose(dt, t.grad)\n","  maxdiff = (dt - t.grad).abs().max().item()\n","  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ZlFLjQyT8PPu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963352092,"user_tz":-240,"elapsed":14,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"10cb293a-2b44-45c0-9f00-61dd7398f7fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["4137\n"]}],"source":["n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","# Note: I am initializating many of these parameters in non-standard ways\n","# because sometimes initializating with e.g. all zeros could mask an incorrect\n","# implementation of the backward pass.\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"QY-y96Y48PPv","executionInfo":{"status":"ok","timestamp":1762963352099,"user_tz":-240,"elapsed":3,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["batch_size = 32\n","n = batch_size # a shorter variable also, for convenience\n","# construct a minibatch\n","ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"8ofj1s6d8PPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963352166,"user_tz":-240,"elapsed":63,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"0974c6f2-2516-47d6-c4a8-8c29408cdff1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.3608, grad_fn=<NegBackward0>)"]},"metadata":{},"execution_count":11}],"source":["# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n","\n","emb = C[Xb] # embed the characters into vectors\n","embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","# Linear layer 1\n","hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n","# BatchNorm layer\n","bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","bndiff = hprebn - bnmeani\n","bndiff2 = bndiff**2\n","bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","bnvar_inv = (bnvar + 1e-5)**-0.5\n","bnraw = bndiff * bnvar_inv\n","hpreact = bngain * bnraw + bnbias\n","# Non-linearity\n","h = torch.tanh(hpreact) # hidden layer\n","# Linear layer 2\n","logits = h @ W2 + b2 # output layer\n","# cross entropy loss (same as F.cross_entropy(logits, Yb))\n","logit_maxes = logits.max(1, keepdim=True).values\n","norm_logits = logits - logit_maxes # subtract max for numerical stability\n","counts = norm_logits.exp()\n","counts_sum = counts.sum(1, keepdims=True)\n","counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","probs = counts * counts_sum_inv\n","logprobs = probs.log()\n","loss = -logprobs[range(n), Yb].mean()\n","\n","# PyTorch backward pass\n","for p in parameters:\n","  p.grad = None\n","for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n","          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n","         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n","         embcat, emb]:\n","  t.retain_grad()\n","loss.backward()\n","loss"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"mO-8aqxK8PPw","executionInfo":{"status":"ok","timestamp":1762963373399,"user_tz":-240,"elapsed":30,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["# Exercise 1: backprop through the whole thing manually,\n","# backpropagating through exactly all of the variables\n","# as they are defined in the forward pass above, one by one\n","\n","dlogprobs = torch.zeros_like(logprobs)# Derivative from the log()?\n","dlogprobs[range(n), Yb] = -1.0/n\n","dprobs = (1.0/probs) * dlogprobs\n","dcounts_sum_inv = (counts * dprobs).sum(1, keepdim = True)\n","dcounts = dcounts_sum_inv * dprobs\n","dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n","dcounts += torch.ones_like(counts) * dcounts_sum # *\n","dnom_logits = (counts) * dcounts\n","dlogits = dnom_logits.clone()\n","dlogit_maxes = (-dnom_logits).sum(1, keepdim=True)\n","dlogits += F.one_hot(logits.max(1).indices, num_classes = logits.shape[1]) * dlogit_maxes\n","dh = dlogits @ W2.T # derivative of matrix multiplication.\n","dW2 = h.T @dlogits\n","db2 = dlogits.sum(0)\n","dhpreact = (1.0 - h**2) * dh\n","dbngain = (bnraw * dhpreact).sum(0, keepdim = True)\n","dbnraw = bngain * dhpreact\n","dbnbias = dhpreact.sum(0, keepdim = True)\n","dbndiff = bnvar_inv * dbnraw\n","dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim = True)\n","dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n","dbndiff2 = (1.0/(n-1) * torch.ones_like(bndiff2) * dbnvar)\n","dbndiff += (2 * bndiff) * dbndiff2\n","dhprebn = dbndiff.clone()\n","dbnmeani = (-dbndiff).sum(0)\n","dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n","dembcat = dhprebn @ W1.T\n","dW1 = embcat.T @ dhprebn\n","db1 = dhprebn.sum(0)\n","demb = dembcat.view(emb.shape)\n","dC = torch.zeros_like(C)\n","for k in range(Xb.shape[0]):\n","  for j in range(Xb.shape[1]):\n","    ix = Xb[k, j]\n","    dC[ix] += demb[k, j]\n","\n","# cmp('logprobs', dlogprobs, logprobs)\n","# cmp('probs', dprobs, probs)\n","# cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n","# cmp('counts_sum', dcounts_sum, counts_sum)\n","# cmp('counts', dcounts, counts)\n","# cmp('norm_logits', dnorm_logits, norm_logits)\n","# cmp('logit_maxes', dlogit_maxes, logit_maxes)\n","# cmp('logits', dlogits, logits)\n","# cmp('h', dh, h)\n","# cmp('W2', dW2, W2)\n","# cmp('b2', db2, b2)\n","# cmp('hpreact', dhpreact, hpreact)\n","# cmp('bngain', dbngain, bngain)\n","# cmp('bnbias', dbnbias, bnbias)\n","# cmp('bnraw', dbnraw, bnraw)\n","# cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n","# cmp('bnvar', dbnvar, bnvar)\n","# cmp('bndiff2', dbndiff2, bndiff2)\n","# cmp('bndiff', dbndiff, bndiff)\n","# cmp('bnmeani', dbnmeani, bnmeani)\n","# cmp('hprebn', dhprebn, hprebn)\n","# cmp('embcat', dembcat, embcat)\n","# cmp('W1', dW1, W1)\n","# cmp('b1', db1, b1)\n","# cmp('emb', demb, emb)\n","# cmp('C', dC, C)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ebLtYji_8PPw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963376436,"user_tz":-240,"elapsed":12,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"d4423611-366d-4f94-b9f0-586c535ce3c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.3607611656188965 diff: 0.0\n"]}],"source":["# Exercise 2: backprop through cross_entropy but all in one go\n","# to complete this challenge look at the mathematical expression of the loss,\n","# take the derivative, simplify the expression, and just write it out\n","\n","# forward pass\n","\n","# before:\n","# logit_maxes = logits.max(1, keepdim=True).values\n","# norm_logits = logits - logit_maxes # subtract max for numerical stability\n","# counts = norm_logits.exp()\n","# counts_sum = counts.sum(1, keepdims=True)\n","# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n","# probs = counts * counts_sum_inv\n","# logprobs = probs.log()\n","# loss = -logprobs[range(n), Yb].mean()\n","\n","# now:\n","loss_fast = F.cross_entropy(logits, Yb)\n","print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"-gCXbB4C8PPx","executionInfo":{"status":"ok","timestamp":1762963378304,"user_tz":-240,"elapsed":36,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["# backward pass\n","\n","# -----------------\n","# YOUR CODE HERE :)\n","dlogits = F.softmax(logits, 1) # TODO. my solution is 3 lines\n","dlogits[range(n), Yb] -= 1\n","dlogits /= n\n","# -----------------\n","\n","#cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"hd-MkhB68PPy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963380151,"user_tz":-240,"elapsed":39,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"fa25ae00-87b2-4eea-9c2b-7488fa797e6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"]}],"source":["# Exercise 3: backprop through batchnorm but all in one go\n","# to complete this challenge look at the mathematical expression of the output of batchnorm,\n","# take the derivative w.r.t. its input, simplify the expression, and just write it out\n","# BatchNorm paper: https://arxiv.org/abs/1502.03167\n","\n","# forward pass\n","\n","# before:\n","# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n","# bndiff = hprebn - bnmeani\n","# bndiff2 = bndiff**2\n","# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","# bnvar_inv = (bnvar + 1e-5)**-0.5\n","# bnraw = bndiff * bnvar_inv\n","# hpreact = bngain * bnraw + bnbias\n","\n","# now:\n","hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n","print('max diff:', (hpreact_fast - hpreact).abs().max())"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"POdeZSKT8PPy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762963394706,"user_tz":-240,"elapsed":72,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"8cb4cb58-1a7a-46f7-d422-a755a69fe814"},"outputs":[{"output_type":"stream","name":"stdout","text":["hprebn          | exact: False | approximate: False | maxdiff: 0.12090827524662018\n"]}],"source":["# backward pass\n","\n","# before we had:\n","# dbnraw = bngain * dhpreact\n","# dbndiff = bnvar_inv * dbnraw\n","# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n","# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n","# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n","# dbndiff += (2*bndiff) * dbndiff2\n","# dhprebn = dbndiff.clone()\n","# dbnmeani = (-dbndiff).sum(0)\n","# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n","\n","# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n","# (you'll also need to use some of the variables from the forward pass up above)\n","\n","# -----------------\n","# YOUR CODE HERE :)\n","dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0)) # TODO. my solution is 1 (long) line\n","# -----------------\n","\n","cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"wPy8DhqB8PPz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762964249151,"user_tz":-240,"elapsed":599236,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"e9ac9a5f-fd48-4396-f451-bfab1b9b9481"},"outputs":[{"output_type":"stream","name":"stdout","text":["12297\n","      0/ 200000: 3.7655\n","  10000/ 200000: 2.1684\n","  20000/ 200000: 2.3930\n","  30000/ 200000: 2.4441\n","  40000/ 200000: 2.0131\n","  50000/ 200000: 2.3207\n","  60000/ 200000: 2.4484\n","  70000/ 200000: 2.0162\n","  80000/ 200000: 2.3376\n","  90000/ 200000: 2.1121\n"," 100000/ 200000: 2.0205\n"," 110000/ 200000: 2.3299\n"," 120000/ 200000: 2.0642\n"," 130000/ 200000: 2.4501\n"," 140000/ 200000: 2.2448\n"," 150000/ 200000: 2.1306\n"," 160000/ 200000: 2.0501\n"," 170000/ 200000: 1.8059\n"," 180000/ 200000: 2.0274\n"," 190000/ 200000: 1.9240\n"]}],"source":["# Exercise 4: putting it all together!\n","# Train the MLP neural net with your own backward pass\n","\n","# init\n","n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True\n","\n","# same optimization as last time\n","max_steps = 200000\n","batch_size = 32\n","n = batch_size # convenience\n","lossi = []\n","\n","# use this context manager for efficiency once your backward pass is written (TODO)\n","with torch.no_grad():\n","\n","  # kick off optimization\n","  for i in range(max_steps):\n","\n","    # minibatch construct\n","    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n","    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n","\n","    # forward pass\n","    emb = C[Xb] # embed the characters into vectors\n","    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","    # Linear layer\n","    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n","    # BatchNorm layer\n","    # -------------------------------------------------------------\n","    bnmean = hprebn.mean(0, keepdim=True)\n","    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n","    bnvar_inv = (bnvar + 1e-5)**-0.5\n","    bnraw = (hprebn - bnmean) * bnvar_inv\n","    hpreact = bngain * bnraw + bnbias\n","    # -------------------------------------------------------------\n","    # Non-linearity\n","    h = torch.tanh(hpreact) # hidden layer\n","    logits = h @ W2 + b2 # output layer\n","    loss = F.cross_entropy(logits, Yb) # loss function\n","\n","    # backward pass\n","    for p in parameters:\n","      p.grad = None\n","    # loss.backward() # use this for correctness comparisons, delete it later!\n","\n","    # manual backprop! #swole_doge_meme\n","    # -----------------\n","    # YOUR CODE HERE :)\n","    dlogits = F.softmax(logits, 1) # TODO. my solution is 3 lines\n","    dlogits[range(n), Yb] -= 1\n","    dlogits /= n\n","\n","    dh = dlogits @ W2.T # derivative of matrix multiplication.\n","    dW2 = h.T @dlogits\n","    db2 = dlogits.sum(0)\n","    dhpreact = (1.0 - h**2) * dh\n","    dbngain = (bnraw * dhpreact).sum(0, keepdim = True)\n","    dbnbias = dhpreact.sum(0, keepdim = True)\n","\n","    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0)) # TODO. my solution is 1 (long) line\n","\n","    dembcat = dhprebn @ W1.T\n","    dW1 = embcat.T @ dhprebn\n","    db1 = dhprebn.sum(0)\n","    demb = dembcat.view(emb.shape)\n","    dC = torch.zeros_like(C)\n","    for k in range(Xb.shape[0]):\n","      for j in range(Xb.shape[1]):\n","        ix = Xb[k, j]\n","        dC[ix] += demb[k, j]\n","    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n","    # -----------------\n","\n","    # update\n","    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n","    for p, grad in zip(parameters, grads):\n","      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n","      p.data += -lr * grad # new way of swole doge TODO: enable\n","\n","    # track stats\n","    if i % 10000 == 0: # print every once in a while\n","      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())\n","\n","    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n","    #   break"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"ZEpI0hMW8PPz","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1762963565582,"user_tz":-240,"elapsed":20,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"59e748e8-f900-4b98-8590-21732ed8641d"},"outputs":[{"output_type":"stream","name":"stdout","text":["(27, 10)        | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n","(30, 200)       | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n","(200,)          | exact: False | approximate: True  | maxdiff: 6.868503987789154e-09\n","(200, 27)       | exact: False | approximate: True  | maxdiff: 2.2351741790771484e-08\n","(27,)           | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n","(1, 200)        | exact: False | approximate: True  | maxdiff: 3.259629011154175e-09\n","(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"]}],"source":["# useful for checking your gradients\n","for p,g in zip(parameters, grads):\n","  cmp(str(tuple(p.shape)), g, p)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"KImLWNoh8PP0","executionInfo":{"status":"ok","timestamp":1762964250107,"user_tz":-240,"elapsed":939,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["# calibrate the batch norm at the end of training\n","\n","with torch.no_grad():\n","  # pass the training set through\n","  emb = C[Xtr]\n","  embcat = emb.view(emb.shape[0], -1)\n","  hpreact = embcat @ W1 + b1\n","  # measure the mean/std over the entire training set\n","  bnmean = hpreact.mean(0, keepdim=True)\n","  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"6aFnP_Zc8PP0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762964251545,"user_tz":-240,"elapsed":1434,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"9555b6cb-d020-419a-8e24-e001d75cf278"},"outputs":[{"output_type":"stream","name":"stdout","text":["train 2.071646213531494\n","val 2.1130552291870117\n"]}],"source":["# evaluate train and val loss\n","\n","@torch.no_grad() # this decorator disables gradient tracking\n","def split_loss(split):\n","  x,y = {\n","    'train': (Xtr, Ytr),\n","    'val': (Xdev, Ydev),\n","    'test': (Xte, Yte),\n","  }[split]\n","  emb = C[x] # (N, block_size, n_embd)\n","  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","  hpreact = embcat @ W1 + b1\n","  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","  h = torch.tanh(hpreact) # (N, n_hidden)\n","  logits = h @ W2 + b2 # (N, vocab_size)\n","  loss = F.cross_entropy(logits, y)\n","  print(split, loss.item())\n","\n","split_loss('train')\n","split_loss('val')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esWqmhyj8PP1","executionInfo":{"status":"aborted","timestamp":1762963352448,"user_tz":-240,"elapsed":10460,"user":{"displayName":"Feix","userId":"00753766162940253556"}}},"outputs":[],"source":["# I achieved:\n","# train 2.0718822479248047\n","# val 2.1162495613098145"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"xHeQNv3s8PP1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762964251559,"user_tz":-240,"elapsed":11,"user":{"displayName":"Feix","userId":"00753766162940253556"}},"outputId":"558d2e23-217f-4388-d237-6bfed2a993b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["carlah.\n","ambril.\n","khi.\n","mri.\n","reigh.\n","salaysie.\n","mahnen.\n","delynn.\n","jareei.\n","ner.\n","kiah.\n","maiivon.\n","leigh.\n","ham.\n","joce.\n","quinn.\n","salin.\n","alvin.\n","quinzelo.\n","dearyn.\n"]}],"source":["# sample from the model\n","g = torch.Generator().manual_seed(2147483647 + 10)\n","\n","for _ in range(20):\n","\n","    out = []\n","    context = [0] * block_size # initialize with all ...\n","    while True:\n","      # forward pass\n","      emb = C[torch.tensor([context])] # (1,block_size,d)\n","      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","      hpreact = embcat @ W1 + b1\n","      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","      h = torch.tanh(hpreact) # (N, n_hidden)\n","      logits = h @ W2 + b2 # (N, vocab_size)\n","      # sample\n","      probs = F.softmax(logits, dim=1)\n","      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n","      context = context[1:] + [ix]\n","      out.append(ix)\n","      if ix == 0:\n","        break\n","\n","    print(''.join(itos[i] for i in out))"]}]}